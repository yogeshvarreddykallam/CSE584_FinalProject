 CSE584_FinalProject

As question sates to focus on science questions , i even tried with math questions to know responses from the llms.

And focued to keep questions as different as possible in each discipline.
Also worked on basic llm such as T5 to well data trained llms gpt4o.

Hereâ€™s a concise and polished GitHub README:



 Faulty Science Question Classification with BERT and RoBERTa

This repository contains the code, dataset, and documentation for a project that explores the classification of faulty science questions using transformerbased large language models (LLMs) such as BERT and RoBERTa. The study focuses on finetuning these models to classify questions containing embedded logical inconsistencies across various disciplines.



 Project Overview

This project investigates the capabilities of LLMs in identifying logically inconsistent (faulty) science questions. It uses a curated dataset with questions from disciplines like physics and mathematics and provides insights into the strengths and limitations of transformerbased models. The key steps include:
 Dataset preprocessing and tokenization.
 Finetuning of BERT and RoBERTa for text classification.
 Evaluation using metrics like accuracy, precision, recall, and F1score.



 Files in the Repository

1. dataset_final.xlsx:
    A curated dataset of faulty science questions.
    Columns include:
      Discipline: Scientific field of the question.
      Question: Faulty science question.
      Reason you think it is faulty: Explanation of the logical inconsistency.
      Which top LLM you tried: Language model tested.
      Response by a top LLM: The response generated by the model.

2. final_code (3).ipynb:
    Preprocessing, finetuning, and evaluation of BERT and RoBERTa models.
    Includes:
      Data cleaning and tokenization.
      Model training with hyperparameter optimization.
      Evaluation with detailed metrics and visualizations.

3. final_code.ipynb:
    Additional experimentation with hyperparameters and crossvalidation.
    Focuses on:
      Optimizing learning rates and batch sizes.
      Comparing models using crossvalidation.




 Run the Notebooks
1. Preprocess the dataset and finetune models using final_code0 (3).ipynb.
2. Experiment with hyperparameter tuning and crossvalidation using final_code0.ipynb.

 Outputs
 Metrics such as accuracy, precision, recall, and F1score.
 Confusion matrices and visualizations for model evaluation.
